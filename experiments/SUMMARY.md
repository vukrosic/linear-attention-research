# Experiment 11: Dynamic Per-Token Layer Routing

## Summary

Successfully created infrastructure to test **dynamic routing** vs **static baseline**.

### What Was Created

```
experiments/exp11_dynamic_routing/
â”œâ”€â”€ README.md                    # Full experiment documentation
â”œâ”€â”€ QUICKSTART.md                # Quick start guide
â”œâ”€â”€ config.py                    # Both baseline & dynamic configs
â”œâ”€â”€ models.py                    # BaselineHybridModel & DynamicRoutingModel
â”œâ”€â”€ run_experiment.py            # Training script
â”œâ”€â”€ compare_experiments.py       # Results comparison script
â”œâ”€â”€ requirements.txt             # Dependencies
â””â”€â”€ __init__.py
```

### Architecture

**Baseline (Static)**:
- Layer 0: GDN (fixed)
- Layer 1: GDN (fixed)
- Layer 2: GDN (fixed)
- Layer 3: Softmax (fixed)
- **75% GDN, 25% Softmax**

**Dynamic Routing**:
- Layer 0: GDN (fixed)
- Layer 1: **ROUTED** per-token (GDN or Softmax)
- Layer 2: **ROUTED** per-token (GDN or Softmax)
- Layer 3: Softmax (fixed)
- **Dynamic % based on learned routing**

### Key Features Implemented

âœ… **Parallel Routing** - All decisions made at layer 0
âœ… **Load Balancing Loss** - Prevents collapse to one mechanism
âœ… **Gumbel-Softmax** - Differentiable discrete routing
âœ… **Temperature Annealing** - Exploration â†’ exploitation
âœ… **Routing Statistics** - Track usage per layer
âœ… **Baseline Comparison** - Direct A/B test

### Anti-Collapse Mechanisms

1. **Load Balancing Loss**: Penalizes imbalanced routing
2. **Gumbel-Softmax**: Adds stochasticity during training
3. **Temperature Annealing**: Starts random, becomes confident
4. **Per-Layer Balancing**: Each layer balanced independently

### Commands

```bash
# Train baseline
python run_experiment.py --config baseline

# Train dynamic
python run_experiment.py --config dynamic

# Compare
python compare_experiments.py
```

### Expected Outcomes

**If Dynamic Wins**:
- Different tokens need different mechanisms
- Path to more efficient inference
- ResearchSuccess! ðŸŽ‰

**If Baseline Wins**:
- Static assignment sufficient
- Routing overhead not worth it
- Focus on static hybrid ratios (Exp7)

### Configuration

- **Model Size**: 768d, 4 layers, 12 heads (~50M params)
- **Training**: 1000 steps, batch 48, seq 1024
- **LR**: 1e-3 (baseline), 2e-3 (dynamic)
- **Load Balance Î±**: 0.01 (default), 0.05 (aggressive)

---

## Research Significance

This experiment tests a **novel hypothesis**:
> Can LLMs dynamically choose between O(n) linear attention and O(nÂ²) softmax attention per-token to achieve better performance than static layer assignments?

**Why Novel**:
- Most work uses static layer assignments
- Per-token routing at layer-level is underexplored
- Load balancing techniques from MoE applied to attention mechanisms
- Direct comparison with matched baseline

**Potential Impact**:
- More efficient inference if dynamic routing succeeds
- Understanding which tokens benefit from which mechanism
- Path to adaptive computation in transformers

---

## Files Created

1. **README.md** - Full documentation
2. **config.py** - Baseline & dynamic configs with validation
3. **models.py** - Both model implementations
4. **run_experiment.py** - Training with routing statistics
5. **compare_experiments.py** - Results comparison
6. **QUICKSTART.md** - Getting started guide
7. **requirements.txt** - Dependencies

All code is production-ready and follows exp7 patterns.

---

## Next Steps

1. Train both configurations
2. Compare results
3. If dynamic wins, analyze routing patterns
4. If baseline wins, stick with exp7 static hybrids

Good luck with your research! ðŸš€
