# Experiment 10: Gated DeltaNet Training Requirements
# Using local FLA clone instead of pip install

# Note: This experiment uses the local flash-linear-attention repository
# No need to install FLA from pip or git
# The local clone is in: flash-linear-attention/fla

# Flash Attention - Required for full transformer baseline
# Note: Install with: pip install flash-attn --no-build-isolation
flash-attn
